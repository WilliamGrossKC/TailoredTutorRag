{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Plan for RAG Context on Syllabuses\n",
    "1. Objective\n",
    "This test aims to determine the optimal chunk size for retrieving the most relevant syllabus information when given a sample question. The system will be evaluated based on its ability to return the correct chunk as the top result or within the top 3 results. We will also record the time taken for retrieval.\n",
    "\n",
    "2. Test Setup\n",
    "2.1 Tools and Libraries:\n",
    "Python: For data processing.\n",
    "Vector database: FAISS, Pinecone, etc.\n",
    "GPT model: To generate realistic queries based on the syllabus content (OpenAI API or similar).\n",
    "Embedding model: Hugging Face’s sentence-transformers, OpenAI embeddings.\n",
    "2.2 Dataset:\n",
    "Use syllabuses of various lengths (e.g., 3000 words for one test case).\n",
    "Text sources: PDFs, DOCX, TXT.\n",
    "2.3 Test Parameters:\n",
    "Chunk sizes: 50, 100, 200 words.\n",
    "Number of test prompts: 20 sample queries per syllabus.\n",
    "Test cases:\n",
    "Analyze the system’s performance for the top 1 and top 3 returned chunks.\n",
    "Record time taken for retrieval.\n",
    "2.4 Metrics to Track:\n",
    "Accuracy: Percentage of times the correct chunk is returned as the top result (or in top 3).\n",
    "Top-1 accuracy: How often the top result matches the expected chunk.\n",
    "Top-3 accuracy: How often the correct chunk is within the top 3 results.\n",
    "Response time: Time taken to retrieve and rank the chunks for each prompt.\n",
    "3. Test Process\n",
    "Step 1: Preprocess Syllabus Data\n",
    "Read syllabuses from various formats (PDF, DOCX, TXT).\n",
    "Split syllabus text into chunks of size 50, 100, 200 words.\n",
    "Sample Python Code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "python\n",
    "from docx import Document\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    return \" \".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size):\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Generate GPT Prompts\n",
    "Use GPT-4 (or similar) to generate sample queries for each syllabus.\n",
    "Label the expected chunk for each query manually.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "python\n",
    "import openai\n",
    "\n",
    "def generate_gpt_prompts(syllabus_text, num_prompts=20):\n",
    "    prompt = f\"Generate {num_prompts} queries based on this syllabus: {syllabus_text[:2000]}...\"\n",
    "    response = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt, max_tokens=1500)\n",
    "    return response[\"choices\"][0][\"text\"].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Embedding and Vectorization\n",
    "Vectorize each chunk of the syllabus and the GPT-generated queries.\n",
    "Store vectors in a vector database.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Initialize embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def embed_chunks(chunks):\n",
    "    return model.encode(chunks)\n",
    "\n",
    "def create_faiss_index(vectors):\n",
    "    d = vectors.shape[1]  # Dimension of embeddings\n",
    "    index = faiss.IndexFlatL2(d)  # Create a FAISS index\n",
    "    index.add(vectors)\n",
    "    return index\n",
    "\n",
    "# Example\n",
    "syllabus_chunks = chunk_text(read_pdf('syllabus.pdf'), 50)  # Example with 50-word chunks\n",
    "chunk_vectors = embed_chunks(syllabus_chunks)\n",
    "faiss_index = create_faiss_index(np.array(chunk_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Query Matching\n",
    "Embed each generated query and use the vector database to find the most similar chunks.\n",
    "Retrieve top-1 and top-3 results for each query.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "python\n",
    "def search_faiss(query, index, chunk_vectors, k=3):\n",
    "    query_vector = model.encode([query])\n",
    "    D, I = index.search(query_vector, k)  # Returns top-k indices\n",
    "    return I[0]  # Indices of top k results\n",
    "\n",
    "# Example\n",
    "query = \"What is the grading policy?\"\n",
    "top_k_results = search_faiss(query, faiss_index, chunk_vectors, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Evaluate Results\n",
    "For each prompt, check if the correct chunk is in the top-1 result or top-3 results.\n",
    "Record the accuracy and retrieval time.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "python\n",
    "import time\n",
    "\n",
    "def evaluate(query, expected_chunk, index, chunk_vectors):\n",
    "    start_time = time.time()\n",
    "    top_results = search_faiss(query, index, chunk_vectors, k=3)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    top1_correct = top_results[0] == expected_chunk\n",
    "    top3_correct = expected_chunk in top_results[:3]\n",
    "    retrieval_time = end_time - start_time\n",
    "    \n",
    "    return top1_correct, top3_correct, retrieval_time\n",
    "\n",
    "# Example Evaluation\n",
    "correct_chunk_id = 5  # Assume we know the correct chunk ID for the query\n",
    "results = evaluate(\"What is the grading policy?\", correct_chunk_id, faiss_index, chunk_vectors)\n",
    "print(f\"Top-1 Accuracy: {results[0]}, Top-3 Accuracy: {results[1]}, Time: {results[2]}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Test Scenarios\n",
    "Scenario 1: Chunk size = 50 words, syllabus length = 3000 words, 20 test prompts.\n",
    "Scenario 2: Chunk size = 100 words, syllabus length = 3000 words, 20 test prompts.\n",
    "Scenario 3: Chunk size = 200 words, syllabus length = 3000 words, 20 test prompts.\n",
    "Each scenario will test:\n",
    "\n",
    "Top-1 Accuracy: What percentage of times the pipeline returned the correct chunk as the top result.\n",
    "Top-3 Accuracy: What percentage of times the correct chunk is within the top 3 results.\n",
    "Retrieval Time: How long the retrieval takes for each chunk size.\n",
    "5. Reporting and Analysis\n",
    "For each scenario, compile the following results:\n",
    "\n",
    "Top-1 Accuracy: (Correct Top-1 Predictions / Total Queries) * 100\n",
    "Top-3 Accuracy: (Correct Top-3 Predictions / Total Queries) * 100\n",
    "Average Retrieval Time: Average time in seconds across all queries.\n",
    "Example Output:\n",
    "\n",
    "Chunk Size\tTop-1 Accuracy (%)\tTop-3 Accuracy (%)\tAvg. Retrieval Time (s)\n",
    "50 Words\t80%\t95%\t0.15\n",
    "100 Words\t85%\t98%\t0.12\n",
    "200 Words\t78%\t90%\t0.10\n",
    "Conclusion\n",
    "By running these scenarios, you will have data to decide on the most efficient chunk size based on accuracy and retrieval time. This methodology ensures that you can scientifically determine the optimal chunk size for your RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Sections of Pipeline, along with potential implementations\n",
    "    1. Document storage only in Dynamo DB\n",
    "        - Only keep documents in Dyanmo DB. Relevant document info loaded via document name search or metadata.\n",
    "    2. Document storage with LLM summary. \n",
    "        - We can store the documents with a LLM summary of the documents that are created when the documents are loaded. This summary can be vectorized and searchd to see if document contains necessary info. \n",
    "        - We have the potential to get creative here, because we could treat each document as its own entity or we can create a kind of file system and summarize every page of every long document.\n",
    "    3. Compute Vector database, store with documents.\n",
    "        - Whenever documents are uploaded, we could trigger a process to compute and upload these vectors. These are then what is sorted.\n",
    "\n",
    "\n",
    "Based on 143 MB of txt files, 100000 pages roughly 250 words a page,\n",
    "5 characters per word, 1 byte per character, 20% extra for spaces and punc,\n",
    "it would be about 70 GB of vector data for the rag context.\n",
    "\n",
    "\n",
    "Notes from meeting with Ahir\n",
    "- Bigger chunks\n",
    "- Lance DB?\n",
    "- Some documents vectorized, some not vectorized. \n",
    "- Assume \n",
    "\n",
    "Some sort of data structure for the chunks\n",
    "    - Mapping\n",
    "    - Metadata of where it came from.\n",
    "\n",
    "Testing Strategy\n",
    " - We are going to take in the syllabuses, and and loop through chunk sizing to see\n",
    " which one is most optimal. We are going to do this by labeling the chunks and seeing\n",
    " which chunk sizing gets the most correct. An example would be for 128 size chucks,\n",
    " we label each chunk and then we send in testing suggestions. We pair these suggestions\n",
    " up with the optimal chunk. If they get it correct, the we know that we are good to go.\n",
    " Maybe I can ask chatGPT to make testing data for me on this. \n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
