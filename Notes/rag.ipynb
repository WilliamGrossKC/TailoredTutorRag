{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PipeLine\n",
    "- need to make a pdf, doc reader, docx, txt, reader\n",
    "- Assume the files themselves are in a dynamno\n",
    "\n",
    "\n",
    "Notes\n",
    "- We have files from a dynamo db database, the files with be pdf, docx, whatever\n",
    "- When stream assistant is called, we will do a keyword search of the data and then grab that chunk and feed it\n",
    "    into the LLM. \n",
    "- Ok this is weird, I need to find the most efficent way of grabbing this data. I can query sort the database.\n",
    "- Where would lance DB fit into this. \n",
    "\n",
    "\n",
    "Potential Pipelines\n",
    "1. Keyword search the document metadata, grab the document, compute vectors on demand, find \n",
    "\n",
    "\n",
    "Sections of Pipeline, along with potential implementations\n",
    "    1. Document storage only in Dynamo DB\n",
    "        - Only keep documents in Dyanmo DB. Relevant document info loaded via document name search or metadata.\n",
    "    2. Document storage with LLM summary. \n",
    "        - We can store the documents with a LLM summary of the documents that are created when the documents are loaded. This summary can be vectorized and searchd to see if document contains necessary info. \n",
    "        - We have the potential to get creative here, because we could treat each document as its own entity or we can create a kind of file system and summarize every page of every long document.\n",
    "    3. Compute Vector database, store with documents.\n",
    "        - Whenever documents are uploaded, we could trigger a process to compute and upload these vectors. These are then what is sorted.\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "- Lance DB\n",
    "- Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example embeddings (just as random vectors for illustration)\n",
    "embedding1 = [0.1, 0.2, 0.3]  # Embedding for sentence 1\n",
    "embedding2 = [0.12, 0.19, 0.31]  # Embedding for a similar sentence\n",
    "embedding3 = [-0.1, 0.3, -0.2]  # Embedding for a different sentence\n",
    "\n",
    "# Calculate cosine similarity between embeddings\n",
    "similarity12 = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "similarity13 = cosine_similarity([embedding1], [embedding3])[0][0]\n",
    "\n",
    "print(f\"Similarity between sentence 1 and 2: {similarity12}\")\n",
    "print(f\"Similarity between sentence 1 and 3: {similarity13}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load BART model and tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# Generation based on retrieved context\n",
    "inputs = tokenizer(\"Context: The syllabus includes...\", return_tensors=\"pt\")\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], max_length=50)\n",
    "print(tokenizer.decode(summary_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Create a FAISS index (e.g., for 768-dimensional embeddings)\n",
    "dimension = 768\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to the index\n",
    "embeddings = np.random.random((1000, dimension)).astype('float32')\n",
    "index.add(embeddings)\n",
    "\n",
    "# Query the index\n",
    "query_embedding = np.random.random((1, dimension)).astype('float32')\n",
    "distances, indices = index.search(query_embedding, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import DenseRetriever, TransformersReader\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "\n",
    "# Create a FAISS document store\n",
    "document_store = FAISSDocumentStore(embedding_dim=768)\n",
    "\n",
    "# Initialize retriever and reader\n",
    "retriever = DenseRetriever(document_store=document_store, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "reader = TransformersReader(model_name_or_path=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "# Build a pipeline\n",
    "pipeline = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for your documents\n",
    "document_embeddings = model.encode([\"Document 1 text\", \"Document 2 text\"])\n",
    "\n",
    "# Generate an embedding for the query\n",
    "query_embedding = model.encode(\"Your query text\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
